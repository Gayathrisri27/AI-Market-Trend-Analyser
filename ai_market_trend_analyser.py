# -*- coding: utf-8 -*-
"""AI Market Trend Analyser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I89zsmHX4bVUm6HuDqCgppWtAgp5ELd3
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from textblob import TextBlob
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import random
from datetime import datetime, timedelta

def fetch_news_articles(keyword, num_articles=10):
    url = f"https://news.google.com/rss/search?q={keyword}&hl=en-US&gl=US&ceid=US:en"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, features="xml")
    articles = soup.findAll('item')

    titles = [article.title.text for article in articles[:num_articles]]
    return titles

def simulate_tweets(keyword, num_tweets=100):
    sentiments = ['positive', 'negative', 'neutral']
    locations = ['New York', 'London', 'Tokyo', 'San Francisco', 'Berlin', 'Sydney']

    simulated_tweets = []
    for _ in range(num_tweets):
        text = f"Simulated tweet about {keyword}: {random.choice(sentiments)} sentiment"
        created_at = datetime.now() - timedelta(days=random.randint(0, 30))
        location = random.choice(locations)
        followers = random.randint(100, 10000)
        simulated_tweets.append({
            'text': text,
            'created_at': created_at,
            'user': {'location': location, 'followers_count': followers, 'screen_name': f"user_{random.randint(1000, 9999)}"}
        })
    return simulated_tweets

def analyze_sentiment(texts):
    sentiments = [TextBlob(text).sentiment.polarity for text in texts]
    return sum(sentiments) / len(sentiments)

def extract_topics(texts):
    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)
    tfidf_matrix = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()  # Updated method

    word_scores = {}
    for col in range(tfidf_matrix.shape[1]):
        word_scores[feature_names[col]] = tfidf_matrix.getcol(col).sum()

    return Counter(word_scores).most_common(10)

def generate_wordcloud(texts):
    text = ' '.join(texts)
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Word Cloud of Market Trends')
    plt.show()

def analyze_trend_over_time(tweets):
    df = pd.DataFrame([(tweet['created_at'], TextBlob(tweet['text']).sentiment.polarity) for tweet in tweets],
                      columns=['date', 'sentiment'])
    df['date'] = pd.to_datetime(df['date'])
    df = df.set_index('date')
    daily_sentiment = df.resample('D').mean()

    plt.figure(figsize=(10, 5))
    plt.plot(daily_sentiment.index, daily_sentiment.sentiment)
    plt.title('Sentiment Trend Over Time')
    plt.xlabel('Date')
    plt.ylabel('Sentiment')
    plt.show()

def geographical_distribution(tweets):
    locations = [tweet['user']['location'] for tweet in tweets if tweet['user']['location']]
    return Counter(locations).most_common(10)

def identify_key_influencers(tweets):
    influencers = [(tweet['user']['screen_name'], tweet['user']['followers_count']) for tweet in tweets]
    return sorted(influencers, key=lambda x: x[1], reverse=True)[:10]

def analyze_market_trends(keyword):
    print(f"Analyzing market trends for: {keyword}")

    # Fetch data
    news_articles = fetch_news_articles(keyword)
    tweets = simulate_tweets(keyword)
    all_texts = news_articles + [tweet['text'] for tweet in tweets]

    # Analyze sentiment
    sentiment = analyze_sentiment(all_texts)
    print(f"Overall sentiment: {sentiment:.2f} (-1 is very negative, +1 is very positive)")

    # Extract topics
    topics = extract_topics(all_texts)
    print("Top topics:")
    for topic, score in topics:
        print(f"- {topic}: {score:.2f}")

    # Generate word cloud
    generate_wordcloud(all_texts)

    # Analyze trend over time
    analyze_trend_over_time(tweets)

    # Geographical distribution of mentions
    geo_distribution = geographical_distribution(tweets)
    print("Geographical distribution of mentions:")
    for location, count in geo_distribution:
        print(f"- {location}: {count}")

    # Identify key influencers
    influencers = identify_key_influencers(tweets)
    print("Key influencers:")
    for name, followers in influencers:
        print(f"- {name}: {followers} followers")

if __name__ == "__main__":
    keyword = input("Enter a keyword to analyze market trends: ")
    analyze_market_trends(keyword)